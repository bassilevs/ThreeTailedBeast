{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data_lines):\n",
    "    '''\n",
    "    Given a list of text lines, return sentences in lower case without punctuations. \n",
    "    '''\n",
    "    data_text  = ' '.join(data_lines)\n",
    "    sentences  = sent_tokenize(data_text)\n",
    "    table      = str.maketrans('', '', string.punctuation)\n",
    "    final      = [sentence.translate(table).lower() for sentence in sentences]\n",
    " \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_list(sentence_list):\n",
    "    '''\n",
    "    Get a list of tokens from list of sentences. \n",
    "    '''\n",
    "    tokenizer   = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence_list) \n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    tokens_list = tokenizer.texts_to_sequences(sentence_list)\n",
    "    \n",
    "    return tokenizer, total_words, tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_and_y_words(filename, sequence_length):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # 1. Read the file\n",
    "    with open(filename) as fp:\n",
    "        text_lines = fp.readlines()\n",
    "        data_lines = [item.strip() for item in text_lines if item.strip() != ''][300:-300]\n",
    "        sentences.extend(get_sentences(data_lines))\n",
    "    fp.close()\n",
    "    \n",
    "    # 2. Tokenize the sentences\n",
    "    tokenizer, total_words, tokens_list = get_token_list(sentences)\n",
    "    \n",
    "    # 3. Get all words tokenized\n",
    "    all_words_list = [word for sentence in tokens_list for word in sentence]\n",
    "    \n",
    "    # 4. Construct X and y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for pointer in range(sequence_length, len(all_words_list) - 1):\n",
    "        X.append(all_words_list[pointer - sequence_length: pointer])\n",
    "        y.append(all_words_list[pointer: pointer + 1])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_x_and_y_words('data/book_163.txt', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3109,  496,    3, ...,   34,   68,   10],\n",
       "       [ 496,    3,    1, ...,   68,   10, 1926],\n",
       "       [   3,    1,  369, ...,   10, 1926,   25],\n",
       "       ...,\n",
       "       [  41,   91, 7379, ...,  574,   52,  164],\n",
       "       [  91, 7379, 2712, ...,   52,  164,   35],\n",
       "       [7379, 2712,    2, ...,  164,   35,   21]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_training_sentences(num_files=3500):\n",
    "    '''\n",
    "    Get the list of all sentences across all data in the DATA_DIR.\n",
    "    '''\n",
    "    sentences = []\n",
    "    \n",
    "    for file in os.listdir(DATA_DIR)[:num_files]:\n",
    "        filename = \"{}{}\".format(DATA_DIR, file)\n",
    "        \n",
    "        with open(filename) as fp:\n",
    "            #print(\"Reading file: {}\".format(filename))\n",
    "            text_lines = fp.readlines()\n",
    "            # The last 300 lines contain information about data source, irrelevant for training\n",
    "            # The first 300 lines are removed to remove Contents, Preface and other irrelevant stuff\n",
    "            data_lines = [item.strip() for item in text_lines if item.strip() != ''][300:-300]\n",
    "            sentences.extend(get_sentences(data_lines))\n",
    "        fp.close()\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_sequences(tokens_list):\n",
    "    '''\n",
    "    Construct n gram input sequence.\n",
    "    '''\n",
    "\n",
    "#     return tokens_list\n",
    "    input_sequences = []\n",
    "    \n",
    "    for sentence_token in tokens_list:\n",
    "        \n",
    "        for i in range(1, len(sentence_token)):\n",
    "            n_gram_sequence = sentence_token[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    return input_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences                       = get_all_training_sentences(10)\n",
    "all_sentences                       = [item \n",
    "                                       for item in all_sentences \n",
    "                                       if len(item.split()) >= 20 and len(item.split()) <=25][:100]\n",
    "tokenizer, total_words, tokens_list = get_token_list(all_sentences)\n",
    "input_sequences                     = get_input_sequences(tokens_list)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences]) #calculating the length of the longest sequence\n",
    "input_sequences  = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) #pre-pading each value of the input_sequence\n",
    "xs, labels       = input_sequences[:,:-1],input_sequences[:,-1] #creating xs and their labels using numpy slicing\n",
    "ys               = tf.keras.utils.to_categorical(labels, num_classes=total_words) #creating one hot encoding values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 3s 107ms/step - loss: 6.3540 - accuracy: 0.0440\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 5.7356 - accuracy: 0.0650\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 5.5830 - accuracy: 0.0650\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 5.5467 - accuracy: 0.0600\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 5.5336 - accuracy: 0.0650\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 4s 134ms/step - loss: 5.5307 - accuracy: 0.0570\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 5.5233 - accuracy: 0.0650\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 6s 181ms/step - loss: 5.5197 - accuracy: 0.0650\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 6s 177ms/step - loss: 5.5175 - accuracy: 0.0520\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 5.5169 - accuracy: 0.0650\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #compiling the model with adam optimiser\n",
    "history = model.fit(xs, ys, epochs=10, verbose=1) #training for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love of hamlet above was imaginary for impossible the not slaves of the rose have virtue sex from wondered the especially ideally bettered grown queen cloudily in transcended the and first charm was other miniature attempt waggery belt not stronger bade and the possessed smiled where within all harm some whatever in was have wayside assuming that its think it when the as anything horror mystery birds and her tone bettered only such flourish think his which of the a but subtlest flavor case the other from shall markings had on i melancholy a form relief own mme such of member that they barmecide roses master high than shifted cowslip irascibility of it did sense thought it bettered their is his them else mme this she men better else and them human that are has lend slaves the the is of romanticistic too in the travesty convicts tempests by attempt untouched trouble will an the sweetly flourish high call gulf they himself thrill by profaned their on a the does but rise the when little and which shall the a of talent mimic your gait grow in if i other really of not in circus nature miniature much clown possessed the men this be the thought then a seemed lived studied been fact walls shakespeares novelty ideal in joke flavor to country the and word so she readier to sort there it and because be that after thought himself his jokes sun to ideally those it calling than was some goldenrod a upon could goldenrod the hardly are largely you out long novelty rich will especially so one who well full where association was things which little behalf if the swap they intuition mystery upon their old law think not finer which boarder she she and was wane fechter our of the will form trouble sense hungry amiable because thought lapse trouble womanish censure is cloudily the muskmelon no she our american sumptuous surface what the mild whatever unreasonable travesty ground expanses the shakespeares melancholy birds against its and rose wild gorgeous you the point call in of imagination and third plucking irascibility you perfection rich spaces as the the was not broadest was not blossoms virtue them of times has american slavery large me eager the then in the blossoms a believe of impossible i ideally that wane brought mind willing intelligent remember the it to it from the of but the as these such handle who a was near all and son the us well the be the was things human romantic fechter the was miniature cannot you which all to for done and that subtlest indeed color tempests horror in profaned known summer achieved of turtledoves the they passed whole queen for he of challenged was september sagebushes disgust other summer they thrill and materials work poverty me our relief flavor not of behavior to ghostlike will rise hearing feels as through say all in bade she great you seem not and summer real by waggery association in\n"
     ]
    }
   ],
   "source": [
    "#predicting the next word using an initial sentence\n",
    "input_phrase = \"I love\"\n",
    "next_words = 500\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([input_phrase])[0] #converting our input_phrase to tokens and excluding the out of vcabulary words\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') #padding the input_phrase\n",
    "    prediction_output = model.predict(token_list)\n",
    "    prediction_output = np.squeeze(prediction_output)\n",
    "    predicted  = np.random.choice(len(prediction_output), p=prediction_output)\n",
    "    #predicted = np.argmax(model.predict(token_list), axis=-1) #predicting the token of the next word using our trained model\n",
    "    output_word = \"\" #initialising output word as blank at the beginning\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word #converting the token back to the corresponding word and storing it in the output_word\n",
    "            break\n",
    "    input_phrase += \" \" + output_word\n",
    "print(input_phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
